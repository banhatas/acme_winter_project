{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Description\n",
    "Tyler Christensen, William Lewis, Addison Powell, Jared Smith\n",
    "\n",
    "## Dataset Descriptions\n",
    "For our project, we intend to explore a few different branches. Each of these require their own dataset due to the nature of the project, though the methodology will remain largely the same. Our two main branches of exploration are:\n",
    "- Sentiment Analysis of Casual Texts\n",
    "    - For this we will use a dataset of over 1.6 million tweets, each labeled with a class related to its sentiment (negative, neutral, or positive).\n",
    "- Sentiment Analysis of Reviews\n",
    "    - For this we will use 2 datasets. The first is a set of yelp reviews from <a href=\"https://www.yelp.com/dataset/documentation/main\">this link</a>. There are 1,250,000 reviews (we took a subset of the larger dataset of 6 million reviews out of concerns for memory), each labeled with a score out of 5 stars. The second is a set of 50,000 IMDb Movie reviews, each labeled with either a positive or negative sentiment.\n",
    "\n",
    "## Validation Set\n",
    "Before we began to look at the data in full, we sealed off 20% of each set to save for a final analysis. This was split via files, and as such we will not be able to access the validation sets unless we specifically load those files in our code. The split was chosen randomly and immediately saved into a separate file. We will reopen these and check our results towards the end of our project.\n",
    "\n",
    "## Data Access\n",
    "All the data can be found in <a href=\"https://drive.google.com/drive/folders/1Hp54gH3TQ93ELuzkHJI76C1nXsSI5CpE?usp=sharing\">this Google Drive folder.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tylerc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "tqdm.pandas()\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag           user  \\\n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  \n",
       "5                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the directory that the data is stored in \n",
    "# don't store in git repo, please store it somewhere else on your computer\n",
    "# data_dir = '../../data/'\n",
    "data_dir = '/home/tylerc/dat/school/acme/'\n",
    "\n",
    "# load the datasets\n",
    "twitter_df = pd.read_csv(data_dir + \"twitter_data.csv\", index_col=0)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv(data_dir + \"imdb_data.csv\", index_col=0)\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>477697</th>\n",
       "      <td>5sHSDjYnNkaia5lUec7rNw</td>\n",
       "      <td>ZSJeZPEoHXMdgMpRIMBpiQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>We have been visiting the Devon Horse Show for...</td>\n",
       "      <td>2019-05-30 14:18:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757770</th>\n",
       "      <td>4dwF1g0wOZjwjxyQ8cRVoA</td>\n",
       "      <td>AjQGanUkM-SFa7MxwTfMRw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>What an amazing, random Friday adventure. I re...</td>\n",
       "      <td>2019-06-21 23:14:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013355</th>\n",
       "      <td>NyPaks2v8GkcWVsCXctpKA</td>\n",
       "      <td>G8r_HHphWNWfRN0LgVwrFA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Last three items broke- they wouldn't return t...</td>\n",
       "      <td>2011-11-14 01:10:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98618</th>\n",
       "      <td>gpzgC3AwKY7cLkMsdSNA3w</td>\n",
       "      <td>2kuhZOrWcLYe_XePccr4lA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Delicious. We got Chicago style. Nice place. O...</td>\n",
       "      <td>2018-03-19 00:28:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446665</th>\n",
       "      <td>Iu1akOzyVihFr7oj9JnK1Q</td>\n",
       "      <td>dvNNkfCyAjOq1HHltSRXRA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Great experience, Roman is the best, he went b...</td>\n",
       "      <td>2016-04-15 17:24:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id             business_id  stars  useful  funny  \\\n",
       "477697   5sHSDjYnNkaia5lUec7rNw  ZSJeZPEoHXMdgMpRIMBpiQ    5.0       2      1   \n",
       "757770   4dwF1g0wOZjwjxyQ8cRVoA  AjQGanUkM-SFa7MxwTfMRw    5.0       2      0   \n",
       "1013355  NyPaks2v8GkcWVsCXctpKA  G8r_HHphWNWfRN0LgVwrFA    1.0       0      0   \n",
       "98618    gpzgC3AwKY7cLkMsdSNA3w  2kuhZOrWcLYe_XePccr4lA    4.0       0      0   \n",
       "446665   Iu1akOzyVihFr7oj9JnK1Q  dvNNkfCyAjOq1HHltSRXRA    5.0       0      0   \n",
       "\n",
       "         cool                                               date  \\\n",
       "477697      2  We have been visiting the Devon Horse Show for...   \n",
       "757770      1  What an amazing, random Friday adventure. I re...   \n",
       "1013355     0  Last three items broke- they wouldn't return t...   \n",
       "98618       0  Delicious. We got Chicago style. Nice place. O...   \n",
       "446665      0  Great experience, Roman is the best, he went b...   \n",
       "\n",
       "                        text  \n",
       "477697   2019-05-30 14:18:56  \n",
       "757770   2019-06-21 23:14:42  \n",
       "1013355  2011-11-14 01:10:28  \n",
       "98618    2018-03-19 00:28:12  \n",
       "446665   2016-04-15 17:24:36  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df = pd.read_csv(data_dir + \"yelp_data.csv\", index_col=0)\n",
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "Due to the size and time constraints from our local machines, we will perform the data transformation on a subset of the twitter data of 10,000 samples. However, the process will be the same for the entire dataset.\n",
    "\n",
    "Since we are treating each sequence of data as 'time series' data, the transformation process requires a few steps.\n",
    "- First, we clean the data of punctuation, numbers, urls, and html tags. We also stop words (words that are so widely used that they contain no useful information)\n",
    "- Next, we create a large corpus of unique words found in the dataset.\n",
    "- Last, we iterate through each word in each sequence and replace it with the index in the corpus. \n",
    "\n",
    "As an example, assume our data is the following two rows:<br>\n",
    "```\n",
    "[\"Hey, I was curious about why you would even think that @Wendys?\",\n",
    "\"Just saw @Dune. I think it was pretty good. What about you?\"]\n",
    "```\n",
    "The first step would transform the data like so:<br>\n",
    "```\n",
    "[['hey', 'i', 'was', 'curious', 'about', 'why', 'you', 'would', 'even', 'think', 'that', 'wendys'],\n",
    " ['just', 'saw', 'dune', 'i', 'think', 'it', 'was', 'pretty', 'good', 'what', 'about', 'you']]\n",
    "```\n",
    "Then, we create a set of unique words:\n",
    "```\n",
    "['hey', 'i', 'was', 'curious', 'about', 'why', \n",
    " 'you', 'would', 'even', 'think', 'that', 'wendys',\n",
    "'just', 'saw', 'dune', 'it', 'was', 'pretty', \n",
    " 'good', 'what']\n",
    "```\n",
    "Last, we replace the transformed data with the indices of the words in the corpus, giving us our ordered sequence of data.\n",
    "```\n",
    "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    " [12, 13, 14, 1, 9, 15, 16, 17, 18, 19, 4, 6]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:30: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/tmp/ipykernel_14494/54894995.py:30: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  return re.split(\"\\W+\", text)\n"
     ]
    }
   ],
   "source": [
    "# Removes punctuation and numbers (by character) and returns as a single string\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if (char not in string.punctuation) and (not char.isdigit())])\n",
    "\n",
    "# Remove URLs from a string\n",
    "def remove_urls(text, replacement_text=\"\"):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(replacement_text, text)\n",
    "\n",
    "# Remove HTML from a string\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "# Splits the message on one or more non-word character\n",
    "# Returns as a list\n",
    "def tokenize(text):\n",
    "    return re.split(\"\\W+\", text)\n",
    "    \n",
    "# Define stopwords and remove them from the list\n",
    "# Also reduce words to the root word\n",
    "def remove_stopwords(text):\n",
    "    stopword_lst = stopwords.words('english')\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in text if word not in stopword_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5889.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning functions to the data\n",
    "twitter_subset = twitter_df.sample(10000)\n",
    "clean_data = twitter_subset['text'].progress_apply(lambda x: remove_stopwords(   # Remove stopwords and shorten to root words\n",
    "                                                tokenize(           # Split message into a list\n",
    "                                                remove_punctuation( # Remove punctuation and numbers\n",
    "                                                remove_urls(        # Remove URLs\n",
    "                                                strip_tags(x)       # Remove HTML tags\n",
    "                                                )).lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9837/10000 [00:03<00:00, 1853.90it/s] "
     ]
    }
   ],
   "source": [
    "# Turn each mesage into a sequence of unique indices\n",
    "#   that correspond to a given word\n",
    "loop = tqdm(total=len(clean_data), position=0, leave=False)\n",
    "\n",
    "words = []\n",
    "for message in clean_data:\n",
    "    words = list(set(words + message))\n",
    "    loop.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:17<00:00, 1853.90it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_data = [[words.index(word) for word in message] for message in clean_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before/After Transformation:\n",
      "and i lost my camera last night \n",
      "['lost', 'camera', 'last', 'night', '']\n",
      "[8478, 1479, 13632, 751, 0]\n"
     ]
    }
   ],
   "source": [
    "i = 5129\n",
    "print(\"Before/After Transformation:\")\n",
    "print(twitter_subset.iloc[i]['text'])\n",
    "print(f\"{clean_data.iloc[i]}\")\n",
    "print(f\"{seq_data[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis\n",
    "- TODO: Any missing values? How to impute?\n",
    "- TODO: Variables to drop?\n",
    "- TODO: What feature engineering is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations and Analysis\n",
    "the distribution of words used should follow the Zipf distribution, even with a smaller lexicon of keywords that exclude things such as particles. See this paper for a Zipf distribution on a X (formerly known as twitter) dataset: https://www.researchgate.net/figure/Zipf-distribution-of-Twitter-keywords-at-different-spatial-levels_fig1_311857596  \n",
    "One thing we expect to see a binomial dist heavily weighted to the negative sentiment for a good/bad sentiment analysis (most people on X (formerly known as twitter) are negative).  \n",
    "If we have pairs of either/or states such as a good/bad sentiment or a funny/serious sentiment we can make predictions on a correlation matrix.  Some visualiztions include identifying keywords and creating a heatmap of the intensity with which they correspond to our hidden states.  Other visualiztions could include lists of words or sentence fragements that correspond the most to certain sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- TODO: What assumptions do our models rely on, and does our data line up with these models? Or does our dataset change what models would be appropriate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
